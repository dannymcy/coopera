<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="COOPERA">
  <meta property="og:title" content="COOPERA"/>
  <meta property="og:description" content="COOPERA"/>
  <meta property="og:url" content="https://dannymcy.github.io/coopera/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="sliders/illum_car_0.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->


  <title>COOPERA</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">COOPERA: Continual Open-Ended Human-Robot Assistance</h1>
              <h2> <font size="+2.5"> Preprint </font></h1>
                <br>

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://dannymcy.github.io" target="_blank">Chenyang Ma</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.cs.ox.ac.uk/people/kai.lu/" target="_blank">Kai Lu</a><sup>1</sup>,</span>
                    </br>
                    <span class="author-block">
                      <a href="https://rutadesai.github.io/" target="_blank">Ruta Desai</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.xavierpuigf.com/" target="_blank">Xavier Puig</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="https://www.cs.ox.ac.uk/people/andrew.markham/" target="_blank">Andrew Markham</a><sup>1</sup>,</span>
                          <span class="author-block">
                            <a href="https://en.wikipedia.org/wiki/Niki_Trigoni" target="_blank">Niki Trigoni</a><sup>1</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block" style="margin-top: 5px;"> <sup>1</sup> University of Oxford<br><sup>2</sup> FAIR, Meta<br></span>
                  </div>
                  <br>
                  

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="Zero_Shot_Task_Hallucination.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2403.13438" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <span class="link-block">
                    <a href="https://github.com/dannymcy/coopera_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4"> -->
        <!-- <video controls autoplay muted loop>
          <source src="static/videos/teaser_vid.mp4" type="video/mp4"> -->
        </video>
          <img src="static/images/teaser.jpg" alt="COOPERA"/>

          <!-- <video controls loop>
            <source src="static/images/teaser_vid.mov">
            Your browser does not support the video tag.
          </video> -->
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        <b>TL;DR:</b> We introduce COOPERA, a novel framework for continual, open-ended human-robot assistance. 
        COOPERA includes (1) simulated humans driven by psychological traits and long-term intentions,
        (2) continuous human feedback that enables the study of long-term, open-ended human-robot collaboration, 
        and (c) a benchmark and approach to personalize the robot's collaborative actions by learning human traits and context-dependent intentions.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->



<!-- Paper abstract -->
<section class="section hero is-light" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            To understand and collaborate with humans, robots must account for individual human traits, habits, and activities over time. 
            However, most robotic assistants lack these abilities, as they primarily focus on predefined tasks in structured environments and 
            lack a human model to learn from. This work introduces <b>COOPERA<b>, a novel framework for <b>CO</b>ntinual, 
            <b>OP</b>en-</b>E</b>nded human-</b>R</b>obot </b>A</b>ssistance, where simulated humans, driven by psychological traits 
            and long-term intentions, interact with robots in complex environments. By integrating continuous human feedback, our framework, 
            for the first time, enables the study of long-term, open-ended human-robot collaboration (HRC) in different collaborative tasks 
            across various time-scales. Within COOPERA, we introduce a benchmark and an approach to personalize the robot's collaborative actions 
            by learning human traits and context-dependent intents. Experiments validate the realism of our simulated humans and demonstrate the value 
            of inferring and personalizing to human intents for open-ended and long-term HRC.          
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->

<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Framework Overview</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/pipeline.jpg" alt="Framework Overview"/>

           <div class="content has-text-justified">
             <p>
              Within COOPERA, the LLM-powered human proposes
              whole-day intentions and tasks, which are executed in the environment. As the robot observes the human actions, it predicts a set of
              tasks to assist them. After each day, the human provides feedback to the robot, enabling the robot to improve for subsequent days.
             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Youtube video -->
<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Human Simulation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/pipeline.jpg" alt="Framework Overview"/>

           <div class="content has-text-justified">
             <p>
              The human-LLM is seeded with an extended profile. At each time of day, the human proposes an intention and decomposes it into tasks, 
              aligning with profile traits and temporal dependence on intention/task history.
               LLM inputs are optimized with Memory Retrieval and Search, and robustness is enhanced via two rounds of Reflexion. 
               This pipeline generates continuous, whole-day intentions and tasks executed in the environment with expressive whole-body motion.
             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Youtube video -->
<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Building an Assistive Agent</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/pipeline.jpg" alt="Human Simulation"/>

           <div class="content has-text-justified">
             <p>
              Our approach decouple robot task inference into two stages: first inferring intentions, then identifying specific tasks. 
              By chaining VLM and classifier, the robot gradually filters and selects tasks that best correlate with the human's traits and 
              temporal context. The robot keeps track of a human profile, inferred from collaboration history. 
              This profile, combined with human feedback, optimizes the robot-VLM through prompting and the classifiers through supervised learning. 
             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->







<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Experiments</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <div class="content has-text-justified">
             <p>
              We provide an extensive empirical study combining multiple off-the-shelf and handcrafted
              datasets, ranging from fundamental spatial questions regarding relative positions and orientations to providing fine-grained 3D information
              on objects' locations, sizes, inclinations, and
              dynamic changes, and plan for robotics tasks
              with full 3D trajectories.
             </p>

              <!-- Experiment 1. -->
              <h2 class="title is-4">Various Forms of Spatial VQA</h2>
                <p>
                  We experiment on the basic form of spatial VQA introduced by SpatialVLM, Intra-Image Object Relations VQA (IaOR-VQA), 
                  as well as two new forms introduced by us: Intra-Image Angular Discrepancies VQA (IaAD-VQA) and Inter-Image Spatial Dynamics VQA (IrSD-VQA).

                  <br><br>In the figure below, we list some sample question and answer pairs generated by our pipeline.
                </p>
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                   <img src="static/images/VQA.jpg" alt="VQA"/>
                </div>
              </div>

              <!-- Experiment 2. -->
              <h2 class="title is-4">Robotics Pick-and-Stack</h2>
                <p>
                  By partially reconstructing the 3D scene with visual alignments, our framework enables VLMs to use
                  tools like rapidly-exploring random tree star (RRT*) to generate accurate, collision-free paths
                  based on task specifications. Given a robot's egocentric observation of a scene with
                  multiple objects, our pipeline uses traditional planning to solve robotics pick-and-stack.
                </p>
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                   <img src="static/images/pick_stack.jpg" alt="pick_stack"/>
                </div>
              </div>


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX"  style="margin-top: 25px; margin-bottom: 25px;">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{ma2024COOPERA,
  title={COOPERA: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors},
  author={Ma, Chenyang and Lu, Kai and Cheng, Ta-Ying and Trigoni, Niki and Markham, Andrew},
  booktitle={Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://ttchengab.github.io/continuous_3d_words/" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
